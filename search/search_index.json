{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"HMAX documentation Welcome to the HMAX documentation. For instructions about how to install it, go here . If you have already downloaded it, you can directly go there . What is HMAX ? HMAX is a computational model based on what is called the ventral-dorsal model in the visal cortex in neurosciences. This model is meant to be biolocically plausible. What are the difference with the original HMAX ? Our implementation include color, sparse coding, parallel computing and GPU computing. References and related work Riesenhuber http://maxlab.neuro.georgetown.edu/hmax Mutch http://cbcl.mit.edu/jmutch/cns About us You can find more on our EEVCOM webpage.","title":"Home"},{"location":"#hmax-documentation","text":"Welcome to the HMAX documentation. For instructions about how to install it, go here . If you have already downloaded it, you can directly go there .","title":"HMAX documentation"},{"location":"#what-is-hmax","text":"HMAX is a computational model based on what is called the ventral-dorsal model in the visal cortex in neurosciences. This model is meant to be biolocically plausible.","title":"What is HMAX ?"},{"location":"#what-are-the-difference-with-the-original-hmax","text":"Our implementation include color, sparse coding, parallel computing and GPU computing.","title":"What are the difference with the original HMAX ?"},{"location":"#references-and-related-work","text":"","title":"References and related work"},{"location":"#riesenhuber","text":"http://maxlab.neuro.georgetown.edu/hmax","title":"Riesenhuber"},{"location":"#mutch","text":"http://cbcl.mit.edu/jmutch/cns","title":"Mutch"},{"location":"#about-us","text":"You can find more on our EEVCOM webpage.","title":"About us"},{"location":"get-started/","text":"Get started This section will show you how run the code for a classification. Prepare your dataset First of all, you must prepare your dataset. HMAX require a training step, so you must have a training and a testing dataset. We recommend to have at least 15 images per class to train the model. In your test dataset, you must have one folder per class, with your images in it. Quick start This will show you how to run HMAX with the default configuration. Computations are not paralyzed or putted on the GPU. You should do this on a small dataset first as it can take some time. 1 . Open the project with Matlab. 2 . Then, you need to train the model with the dataset you have prepared hmax('../dataset/train/', 'Train', true); 3 . Run it on your test dataset hmax('../dataset/test/'); This will read recursively the ../dataset/test/ folder architecture and create a similar one in the ./result folder. Each images will give a Matlab binary file containing the HMAX encoding. 4 . If your folders are named corresponding to the class name of the images, then you can run the following command: T = classifier.loadC2s('./results/'); Where T is a table containing HMAX encodings with corresponding labels. Machine Learning Once you've done the step 4. , you can use the data in a Machine Learning tool. The recommendation is to split the data ( T ) in two, one with 80% of the observations for training and the other with the rest of the 20% for validation. [Ttrain Ttest] = classifier.splitdata(T, 0.8); Then, you can use the Classification Learner toolbox to create a model. For our tests, we used a Linear SVM without PCA. Finally, we can test our model averagePrecision = classifier.runtest(trainedModel, Ttest) Go further Now that you know how to run HMAX, we will see how to run the code with GPU, parallel computing, with your own parameters, etc ...","title":"Get started"},{"location":"get-started/#get-started","text":"This section will show you how run the code for a classification.","title":"Get started"},{"location":"get-started/#prepare-your-dataset","text":"First of all, you must prepare your dataset. HMAX require a training step, so you must have a training and a testing dataset. We recommend to have at least 15 images per class to train the model. In your test dataset, you must have one folder per class, with your images in it.","title":"Prepare your dataset"},{"location":"get-started/#quick-start","text":"This will show you how to run HMAX with the default configuration. Computations are not paralyzed or putted on the GPU. You should do this on a small dataset first as it can take some time. 1 . Open the project with Matlab. 2 . Then, you need to train the model with the dataset you have prepared hmax('../dataset/train/', 'Train', true); 3 . Run it on your test dataset hmax('../dataset/test/'); This will read recursively the ../dataset/test/ folder architecture and create a similar one in the ./result folder. Each images will give a Matlab binary file containing the HMAX encoding. 4 . If your folders are named corresponding to the class name of the images, then you can run the following command: T = classifier.loadC2s('./results/'); Where T is a table containing HMAX encodings with corresponding labels.","title":"Quick start"},{"location":"get-started/#machine-learning","text":"Once you've done the step 4. , you can use the data in a Machine Learning tool. The recommendation is to split the data ( T ) in two, one with 80% of the observations for training and the other with the rest of the 20% for validation. [Ttrain Ttest] = classifier.splitdata(T, 0.8); Then, you can use the Classification Learner toolbox to create a model. For our tests, we used a Linear SVM without PCA. Finally, we can test our model averagePrecision = classifier.runtest(trainedModel, Ttest)","title":"Machine Learning"},{"location":"get-started/#go-further","text":"Now that you know how to run HMAX, we will see how to run the code with GPU, parallel computing, with your own parameters, etc ...","title":"Go further"},{"location":"indeep/","text":"In deep In this section, we will show how to use advanced parameters with HMAX. If you haven't read yet, you should start by the Get started section. How to specify parameters There is two way to pass parameters to the program: Directly through the Matlab command From a configuration file The priority is: 1. Parameters given to the command 2. Parameters written in the configuration file 3. Parameters set internally That's why in the Get started section, we wrote this: hmax('../dataset/train/', 'Train', true); The Train parameters is set to false internally and in the default configuration file. But as we give the value true as an argument, it will override the default value. Avalaible parameters Name Description Type expected Default value ConfigurationFile Custom configuration file String - JSON file \"./defaultParameters.json\" Output Custom output directory String - Directory \"./results\" GPU Use GPU for computation Boolean false Parallel Use parallel computing Boolean false Engine Specify an algorithm to use String \"sparseCodingColor\" ImageSize Resize image to this size in pixels Integer 140 GaborNbOrientations Integer 4 GaborAspectRatio Aspect ratio of Gabor filters Float 0.3 GaborEffectiveWidth Width of Gabor filters Float[] [2.8, 3.6, 4.5, ..., 18.2] GaborWavelength Wavelength of Gabor filters Float[] [3.5, 4.6, 5.6, ..., 22.8] GaborSizes Sizes of Gabor filters Integer[] [7, 9, 11, 13, ..., 39] FiltersSizes Integer[] [4, 8, 12, 16] MaxPoolingSizes Size of the max pooling Integer[] [8, 8, 10, 10, ..., 22] ColorNbChannels Number of color channels Integer 4 SparseCodingFilterSize Integer 12 SparseCodingNbPatches Integer 10000 SparseCodingBatchSize Integer 1000 SparseCodingNbIterations Integer 2 SparseCodingPenalty Float 0.2 NbFilters Integer 1000 Train Train the model Boolean false Engine Choose between: classic , sparseCoding , color or sparseColor ImageSize The program will alway resize input images, so the size will be uniform. Bigger they are, slower the execution will be. GaborEffectiveWidth Default values: [2.8, 3.6, 4.5, 5.4, 6.3, 7.3, 8.2, 9.2, 10.2, 11.3, 12.3, 13.4, 14.6, 15.8, 17.0, 18.2] GaborWavelength Default values: [3.5, 4.6, 5.6, 6.8, 7.9, 9.1, 10.3, 11.5, 12.7, 14.1, 15.4, 16.8, 18.2, 19.7, 21.2, 22.8] GaborFilterSizes Default values: [7, 9, 11, 13, 15, 17, 19, 21, 23, 25, 27, 29, 31, 33, 35, 37] MaxPoolingSizes Default values: [8, 8, 10, 10, 12, 12, 14, 14, 16, 16, 18, 18, 20, 20, 22, 22]","title":"In deep"},{"location":"indeep/#in-deep","text":"In this section, we will show how to use advanced parameters with HMAX. If you haven't read yet, you should start by the Get started section.","title":"In deep"},{"location":"indeep/#how-to-specify-parameters","text":"There is two way to pass parameters to the program: Directly through the Matlab command From a configuration file The priority is: 1. Parameters given to the command 2. Parameters written in the configuration file 3. Parameters set internally That's why in the Get started section, we wrote this: hmax('../dataset/train/', 'Train', true); The Train parameters is set to false internally and in the default configuration file. But as we give the value true as an argument, it will override the default value.","title":"How to specify parameters"},{"location":"indeep/#avalaible-parameters","text":"Name Description Type expected Default value ConfigurationFile Custom configuration file String - JSON file \"./defaultParameters.json\" Output Custom output directory String - Directory \"./results\" GPU Use GPU for computation Boolean false Parallel Use parallel computing Boolean false Engine Specify an algorithm to use String \"sparseCodingColor\" ImageSize Resize image to this size in pixels Integer 140 GaborNbOrientations Integer 4 GaborAspectRatio Aspect ratio of Gabor filters Float 0.3 GaborEffectiveWidth Width of Gabor filters Float[] [2.8, 3.6, 4.5, ..., 18.2] GaborWavelength Wavelength of Gabor filters Float[] [3.5, 4.6, 5.6, ..., 22.8] GaborSizes Sizes of Gabor filters Integer[] [7, 9, 11, 13, ..., 39] FiltersSizes Integer[] [4, 8, 12, 16] MaxPoolingSizes Size of the max pooling Integer[] [8, 8, 10, 10, ..., 22] ColorNbChannels Number of color channels Integer 4 SparseCodingFilterSize Integer 12 SparseCodingNbPatches Integer 10000 SparseCodingBatchSize Integer 1000 SparseCodingNbIterations Integer 2 SparseCodingPenalty Float 0.2 NbFilters Integer 1000 Train Train the model Boolean false","title":"Avalaible parameters"},{"location":"indeep/#engine","text":"Choose between: classic , sparseCoding , color or sparseColor","title":"Engine"},{"location":"indeep/#imagesize","text":"The program will alway resize input images, so the size will be uniform. Bigger they are, slower the execution will be.","title":"ImageSize"},{"location":"indeep/#gaboreffectivewidth","text":"Default values: [2.8, 3.6, 4.5, 5.4, 6.3, 7.3, 8.2, 9.2, 10.2, 11.3, 12.3, 13.4, 14.6, 15.8, 17.0, 18.2]","title":"GaborEffectiveWidth"},{"location":"indeep/#gaborwavelength","text":"Default values: [3.5, 4.6, 5.6, 6.8, 7.9, 9.1, 10.3, 11.5, 12.7, 14.1, 15.4, 16.8, 18.2, 19.7, 21.2, 22.8]","title":"GaborWavelength"},{"location":"indeep/#gaborfiltersizes","text":"Default values: [7, 9, 11, 13, 15, 17, 19, 21, 23, 25, 27, 29, 31, 33, 35, 37]","title":"GaborFilterSizes"},{"location":"indeep/#maxpoolingsizes","text":"Default values: [8, 8, 10, 10, 12, 12, 14, 14, 16, 16, 18, 18, 20, 20, 22, 22]","title":"MaxPoolingSizes"},{"location":"installation/","text":"Installation HMAX is written in Matlab . Matlab is a proprietary software so you must have a Matlab licence. Plus, in order to run HMAX, you must have theses Toolboxes: Requirement Matlab >2017 Optimization toolbox Parallel Computing Toolbox (optional) Global Optimization Toolbox (sparse coding only) Statistics and Machine Learning Toolbox (classification) Then download the code through git: git clone https://github.com/EEVCOM-Montpellier/HMAX Or download it directly: https://github.com/EEVCOM-Montpellier/HMAX/archive/master.zip","title":"Installation"},{"location":"installation/#installation","text":"HMAX is written in Matlab . Matlab is a proprietary software so you must have a Matlab licence. Plus, in order to run HMAX, you must have theses Toolboxes: Requirement Matlab >2017 Optimization toolbox Parallel Computing Toolbox (optional) Global Optimization Toolbox (sparse coding only) Statistics and Machine Learning Toolbox (classification) Then download the code through git: git clone https://github.com/EEVCOM-Montpellier/HMAX Or download it directly: https://github.com/EEVCOM-Montpellier/HMAX/archive/master.zip","title":"Installation"},{"location":"methodology-and-results/","text":"Methodology and results The HMAX efficiency and its different extensions available in literature are compare through the Caltech101 dataset, which you can find here . This dataset contains 102 categories, each one including between 31 and more than 800 images. As we want a multi-class classifier, we need a balanced dataset. That's why we only take the first 31 images for all the categories. For the HMAX training, we take the first 15 images of each categories, due to Serre et al. implementation. This is an issue because half of our testing images are present in the training stage. That's why in a second time, we created an alternative dataset in witch training images were taken randomly in the complete dataset. HMAX is demanding in resources and in time and in order to quickly test the algorithm in a reasonable amount of time, so we created a subset dataset that we called Caltech 20. It contains theses categories: Airplanes Electric guitar Hedgehog Minaret Binocular Emu Joshua tree Pigeon Chandelier Faces Laptop Rooster Dolphin Gerenuk Mandolin Sea horse Strawberry Trilobite Water lilly Windsor chair Comparison with Serre et al. algorithm - Caltech 20 Renoult et al. Serre et al. Renoult et al. Serre et al. Renoult et al. Engine Classic Gray Color Color DO Sparse coding Correct classification rate 57.14% 58.57% 64.29% 50.71% 45.71% Execution time 57m25s 1h16m16s 13h28m22s 11h09m34s 2h48m00s Comparison with Serre et al. algorithm - Caltech 101 Renoult et al. Serre et al. Renoult et al. Serre et al. Renoult et al. Engine Classic Gray Color Color DO Sparse coding Correct classification rate 31.79% 27.45% 29.55% 25.63% --.--% Execution time 4h38m33s 6h4m8s 2d21h22m2s 2d4h11m30s -h--m--s","title":"Methodology and results"},{"location":"methodology-and-results/#methodology-and-results","text":"The HMAX efficiency and its different extensions available in literature are compare through the Caltech101 dataset, which you can find here . This dataset contains 102 categories, each one including between 31 and more than 800 images. As we want a multi-class classifier, we need a balanced dataset. That's why we only take the first 31 images for all the categories. For the HMAX training, we take the first 15 images of each categories, due to Serre et al. implementation. This is an issue because half of our testing images are present in the training stage. That's why in a second time, we created an alternative dataset in witch training images were taken randomly in the complete dataset. HMAX is demanding in resources and in time and in order to quickly test the algorithm in a reasonable amount of time, so we created a subset dataset that we called Caltech 20. It contains theses categories: Airplanes Electric guitar Hedgehog Minaret Binocular Emu Joshua tree Pigeon Chandelier Faces Laptop Rooster Dolphin Gerenuk Mandolin Sea horse Strawberry Trilobite Water lilly Windsor chair","title":"Methodology and results"},{"location":"methodology-and-results/#comparison-with-serre-et-al-algorithm-caltech-20","text":"Renoult et al. Serre et al. Renoult et al. Serre et al. Renoult et al. Engine Classic Gray Color Color DO Sparse coding Correct classification rate 57.14% 58.57% 64.29% 50.71% 45.71% Execution time 57m25s 1h16m16s 13h28m22s 11h09m34s 2h48m00s","title":"Comparison with Serre et al. algorithm - Caltech 20"},{"location":"methodology-and-results/#comparison-with-serre-et-al-algorithm-caltech-101","text":"Renoult et al. Serre et al. Renoult et al. Serre et al. Renoult et al. Engine Classic Gray Color Color DO Sparse coding Correct classification rate 31.79% 27.45% 29.55% 25.63% --.--% Execution time 4h38m33s 6h4m8s 2d21h22m2s 2d4h11m30s -h--m--s","title":"Comparison with Serre et al. algorithm - Caltech 101"},{"location":"visualization/","text":"Visualization For visualize the different activations cards, you can use functions available in the helpers.display package.","title":"Visualization"},{"location":"visualization/#visualization","text":"For visualize the different activations cards, you can use functions available in the helpers.display package.","title":"Visualization"}]}